{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srividhyak2011/Demo-Datascienceproject/blob/main/M5_MP6_NB_Corona_tweets_Classification_of_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Va8_ShN-EKe"
      },
      "source": [
        "# Advanced Data Science and Machine Intelligence\n",
        "## A Program by IITMadras and TalentSprint\n",
        "### Mini Project Notebook 06 : Classification of Coronavirus Tweets using NLP Techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Z3UnO-PtYwaB"
      },
      "source": [
        "#@title Download Dataset\n",
        "%%capture\n",
        "!wget https://cdn.extras.talentsprint.com//DLFA/Experiment_related_data/corona_nlp_test.csv\n",
        "!wget https://cdn.extras.talentsprint.com//DLFA/Experiment_related_data/corona_nlp_train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "palette=sns.color_palette('magma')\n",
        "sns.set(palette=palette)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional, Dense, Dropout, Embedding, SpatialDropout1D\n",
        "\n",
        "from keras.models import Sequential   # the model"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##  **Data Loading and Perform Exploratory Data Analysis** (2 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n",
        "  - Use encoding='ISO-8859-1' when reading csv files\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values and impute/drop them"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Data Pre-Processing**  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n",
        "\n",
        "- Use regex package for cleaning the punctuations and unnecessary symbols\n",
        "- Encode the categories of tweets i.e. Sentiment column of dataset\n",
        "- Drop the columns which are not useful for the classification\n",
        "task\n",
        "- Remove stop words (Stop words are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text.\n",
        "- Tokenize the sentences\n",
        "- Set the MAX_NB_WORDS of tokenizer method in keras\n",
        "- Set MAX_SEQUENCE_LENGTH for padding/trimming sequences of fixed length\n",
        "-  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation)** (3 Points)\n",
        "\n",
        "- Download and load  word embeddings  \n",
        "- Create a weight matrix for words in the training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Word embeddings\n",
        "\n",
        "!wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "cellView": "form",
        "id": "maUyBFDAOSQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Build and Train the Deep Recurrent Model using RNN/LSTM** (2 points)\n",
        "\n",
        "- Create and Embedding layer to feed to LSTM\n",
        "    - input_dim: Size of the vocabulary\n",
        "    - output_dim: Dimension of the dense embedding\n",
        "\n",
        "- The ouput of embedding layers is fed to LSTM\n",
        "- Compile the model,train and tweak the hyperparameters\n",
        "- Calculate the train set accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Evaluate the Model and get model predictions on the test dataset** (1 Points)\n",
        "\n",
        "* Evaluate the model on test set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}