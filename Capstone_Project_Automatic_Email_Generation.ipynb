{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcBZfAcrdqsOAHgk6Db8hJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srividhyak2011/Demo-Datascienceproject/blob/main/Capstone_Project_Automatic_Email_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9V3AHQhjicj",
        "outputId": "3311c8b1-4ec5-4576-97b1-c4e004a8e503"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJ8slIDOc1o",
        "outputId": "9077a565-f329-493e-be3c-207b7eb17107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries.\n",
            "I have the following questions:\n",
            "NUMBER.\n",
            "Should I have the following groceries delivered to my house?\n",
            "NUMBER.\n",
            "Should I have the following groceries delivered to my house?\n",
            "NUMBER.\n",
            "Should I\n",
            "Word Error Rate using gpt2  (WER): 2.5\n",
            "Character Error Rate gpt2 (CER): 2.921875\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_tag = \"postbot/gpt2-medium-emailgen\"\n",
        "generator = pipeline(\n",
        "              'text-generation',\n",
        "              model=model_tag,\n",
        "            )\n",
        "\n",
        "prompt = \"\"\"\n",
        "Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries.\"\"\"\n",
        "\n",
        "result = generator(\n",
        "    prompt,\n",
        "    max_length=64,\n",
        "    do_sample=False,\n",
        "    early_stopping=True,\n",
        ") # generate\n",
        "print(result[0]['generated_text'])\n",
        "\n",
        "import Levenshtein\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return Levenshtein.distance(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "    return Levenshtein.distance(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Example usage:\n",
        "reference_text = \"\"\"\n",
        "Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries.\"\"\"\n",
        "hypothesis_text = result[0]['generated_text']\n",
        "\n",
        "wer = calculate_wer(reference_text, hypothesis_text)\n",
        "cer = calculate_cer(reference_text, hypothesis_text)\n",
        "\n",
        "print(f\"Word Error Rate using gpt2 Dataset (WER): {wer}\")\n",
        "print(f\"Character Error Rate using gpt2 Dataset (CER): {cer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_tag = \"pszemraj/opt-350m-email-generation\"\n",
        "generator = pipeline(\n",
        "              'text-generation',\n",
        "              model=model_tag,\n",
        "              use_fast=False,\n",
        "              do_sample=False,\n",
        "              early_stopping=True,\n",
        "            )\n",
        "\n",
        "prompt = \"\"\"\n",
        "Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries\"\"\"\n",
        "\n",
        "result = generator(\n",
        "    prompt,\n",
        "    max_length=64,\n",
        ") # generate\n",
        "print(result[0]['generated_text'])\n",
        "\n",
        "import Levenshtein\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return Levenshtein.distance(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "    return Levenshtein.distance(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Example usage:\n",
        "reference_text = \"\"\"\n",
        "Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries.\"\"\"\n",
        "hypothesis_text = result[0]['generated_text']\n",
        "\n",
        "wer = calculate_wer(reference_text, hypothesis_text)\n",
        "cer = calculate_cer(reference_text, hypothesis_text)\n",
        "\n",
        "print(f\"Word Error Rate using Facebook dataset  (WER): {wer}\")\n",
        "print(f\"Character Error Rate using Facebook dataset (CER): {cer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCacyDYZPujo",
        "outputId": "3b1603aa-cc19-42b2-b13c-e0f4113457b4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries at the Bisti supermarket in Paris.\n",
            "I am looking for the price of the grocery at the Bisti supermarket in Paris.\n",
            "I am looking for the price of the grocery at the Bisti supermarket in Paris.\n",
            "Word Error Rate using Facebook dataset  (WER): 3.0833333333333335\n",
            "Character Error Rate Facebook dataset (CER): 2.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XclYsEE7RAQr",
        "outputId": "c15f1639-0e82-4ab5-d385-ce8d616872a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_email(prompt_text):\n",
        "    # Tokenize the prompt\n",
        "    tokenized_prompt = tokenizer.encode(prompt_text, return_tensors='pt')\n",
        "\n",
        "    # Mask the last word in the prompt\n",
        "    masked_index = torch.where(tokenized_prompt == tokenizer.mask_token_id)[1]\n",
        "    tokenized_prompt[0, masked_index] = tokenizer.mask_token_id\n",
        "\n",
        "    # Generate the masked token prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokenized_prompt)\n",
        "        predictions = outputs.logits[0, masked_index, :]\n",
        "\n",
        "    predicted_indexes = torch.topk(predictions, 5, dim=-1).indices.tolist()[0]\n",
        "\n",
        "    generated_texts = []\n",
        "    for index in predicted_indexes:\n",
        "        # Replace the masked token with predicted token\n",
        "        predicted_token = tokenizer.decode([index]).strip()\n",
        "        generated_text = prompt_text.replace(tokenizer.mask_token, predicted_token)\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Prompt for the email content\n",
        "prompt = \"\"\"Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries [MASK] \"\"\"\n",
        "\n",
        "generated_emails = generate_email(prompt)\n",
        "\n",
        "print(\"Generated Emails:\")\n",
        "for i, email in enumerate(generated_emails, start=1):\n",
        "    print(f\"Email {i}:\")\n",
        "    print(email)\n",
        "    print(\"\\n\")\n",
        "    import Levenshtein\n",
        "    def calculate_wer(reference, hypothesis):\n",
        "        reference_words = reference.split()\n",
        "        hypothesis_words = hypothesis.split()\n",
        "        return Levenshtein.distance(reference_words, hypothesis_words) / len(reference_words)\n",
        "    def calculate_cer(reference, hypothesis):\n",
        "        return Levenshtein.distance(reference, hypothesis) / len(reference)\n",
        "    # Example usage:\n",
        "    reference_text = \"\"\"\n",
        "    Dear Sir,\n",
        "\n",
        "    I would like to enquire about rate of the groceries.\"\"\"\n",
        "    hypothesis_text = email\n",
        "    wer = calculate_wer(reference_text, hypothesis_text)\n",
        "    cer = calculate_cer(reference_text, hypothesis_text)\n",
        "    print(f\"Word Error Rate using BERT MODEL (WER): {wer}\")\n",
        "    print(f\"Character Error Rate using BERT MODEL (CER): {cer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3bD2umhbPxM",
        "outputId": "c4ace29c-c08f-46d3-c337-cae9c3c64248"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Emails:\n",
            "Email 1:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries . \n",
            "\n",
            "\n",
            "Word Error Rate using BERT MODEL (WER): 0.16666666666666666\n",
            "Character Error Rate using BERT MODEL (CER): 0.1527777777777778\n",
            "Email 2:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries ; \n",
            "\n",
            "\n",
            "Word Error Rate using BERT MODEL (WER): 0.16666666666666666\n",
            "Character Error Rate using BERT MODEL (CER): 0.16666666666666666\n",
            "Email 3:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries ! \n",
            "\n",
            "\n",
            "Word Error Rate using BERT MODEL (WER): 0.16666666666666666\n",
            "Character Error Rate using BERT MODEL (CER): 0.16666666666666666\n",
            "Email 4:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries ? \n",
            "\n",
            "\n",
            "Word Error Rate using BERT MODEL (WER): 0.16666666666666666\n",
            "Character Error Rate using BERT MODEL (CER): 0.16666666666666666\n",
            "Email 5:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries ... \n",
            "\n",
            "\n",
            "Word Error Rate using BERT MODEL (WER): 0.16666666666666666\n",
            "Character Error Rate using BERT MODEL (CER): 0.18055555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'  # You can change the size here\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Prompt for the email content\n",
        "#prompt = \"Dear [MASK], I hope this email finds you well. I wanted to reach out and discuss\"\n",
        "prompt = \"\"\"Dear Sir,\n",
        "\n",
        "I would like to enquire about rate of the groceries\"\"\"\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate the email text\n",
        "output = model.generate(input_ids, max_length=150, num_return_sequences=1, temperature=0.7, top_k=50)\n",
        "\n",
        "generated_emails = []\n",
        "for i, sample_output in enumerate(output):\n",
        "    generated_emails.append(tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "\n",
        "print(\"Generated Emails:\")\n",
        "for i, email in enumerate(generated_emails, start=1):\n",
        "    print(f\"Email {i}:\")\n",
        "    print(email)\n",
        "    print(\"\\n\")\n",
        "    import Levenshtein\n",
        "    def calculate_wer(reference, hypothesis):\n",
        "        reference_words = reference.split()\n",
        "        hypothesis_words = hypothesis.split()\n",
        "        return Levenshtein.distance(reference_words, hypothesis_words) / len(reference_words)\n",
        "    def calculate_cer(reference, hypothesis):\n",
        "        return Levenshtein.distance(reference, hypothesis) / len(reference)\n",
        "    # Example usage:\n",
        "    reference_text = \"\"\"\n",
        "    Dear Sir,\n",
        "\n",
        "    I would like to enquire about rate of the groceries.\"\"\"\n",
        "    hypothesis_text = email\n",
        "    wer = calculate_wer(reference_text, hypothesis_text)\n",
        "    cer = calculate_cer(reference_text, hypothesis_text)\n",
        "    print(f\"Word Error Rate using GPT2 MODEL (WER): {wer}\")\n",
        "    print(f\"Character Error Rate using GPT2 MODEL (CER): {cer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7m3rEp7feSe",
        "outputId": "06441e68-7ba7-45af-a659-2478823d2595"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Emails:\n",
            "Email 1:\n",
            "Dear Sir,\n",
            "\n",
            "I would like to enquire about rate of the groceries.\n",
            "\n",
            "I have a large family, and I am very fond of my children. I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children.\n",
            "\n",
            "I have a large family, and I am very fond of my children\n",
            "\n",
            "\n",
            "Word Error Rate using GPT2 MODEL (WER): 8.666666666666666\n",
            "Character Error Rate using GPT2 MODEL (CER): 6.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return Levenshtein.distance(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "    return Levenshtein.distance(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Example usage:\n",
        "reference_text = \"The quick brown fox\"\n",
        "hypothesis_text = \"The quik brown fox\"\n",
        "\n",
        "wer = calculate_wer(reference_text, hypothesis_text)\n",
        "cer = calculate_cer(reference_text, hypothesis_text)\n",
        "\n",
        "print(f\"Word Error Rate (WER): {wer}\")\n",
        "print(f\"Character Error Rate (CER): {cer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTDGAqt6jpNq",
        "outputId": "175c7f97-ee1b-4ddb-acb9-7f1c4b714f9f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate (WER): 0.25\n",
            "Character Error Rate (CER): 0.05263157894736842\n"
          ]
        }
      ]
    }
  ]
}