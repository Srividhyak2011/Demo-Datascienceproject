{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srividhyak2011/Demo-Datascienceproject/blob/main/M2_AST_06_Naive_Bayes_and_Decision_Trees_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powered-thong"
      },
      "source": [
        "# Applied Data Science and Machine Intelligence\n",
        "## A program by IITM and TalentSprint\n",
        "### Assignment 06: Naive Bayes Algorithm & Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innocent-wiring"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actual-tiger"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand the Bayes theorem.\n",
        "* use the Bayes theorem in the Naive Bayes algorithm for classification.\n",
        "* use different metrics for the Naive bayes algorithm\n",
        "* understand the basics of Decision trees.\n",
        "* use multiple metrics which are popular with the decision tree algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIL-FBcvmDB"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8ptIQOnnbwbz"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M2_AST_06_Naive_Bayes_and_Decision_Trees_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://adsmi-iitm.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prR7F6CMoaTC"
      },
      "source": [
        "### Import important libraries required for this assignments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKrMcAub09Pv"
      },
      "source": [
        "import numpy as np \n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap                                    # to import color map                                          \n",
        "from sklearn import datasets\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor          # to import DT classifier and Regressor\n",
        "import graphviz                                                                 # to import graphviz \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ncCmfsYmX_"
      },
      "source": [
        "### Bayes Theorem\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdAbsCMrWWbK"
      },
      "source": [
        "**Why we need Bayesian inference?**\n",
        "\n",
        "In general, statistical inference is the process of determining properties of a model/distribution, given some data. Bayesian inference can be seen as the Bayesian counterpart to frequentist inference. In Frequentist inference, there is usually the notion of some true, unknown, parameter which is a constant, and point estimates are inferred from data. Contrarily, Bayesian inference treats the model parameters as random variables and usually wants to deduce probabilistic statements about the distribution of parameters.\n",
        "\n",
        "**Terminology**\n",
        "\n",
        "The basic terms related to Bayesian inference are as follows:\n",
        "\n",
        "- **Prior:** the probability distribution that would express one's beliefs about an uncertain quantity before some evidence is taken into account.\n",
        "- **Posterior:**  in Bayesian statistics, it is the revised or updated probability of an event occurring after taking into consideration new information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qTjWIGu6PU4Q"
      },
      "source": [
        "### Naive Bayes Classifiers\n",
        "\n",
        "Naive Bayes classifiers are built on Bayes rule.\n",
        "We are try to use bayes theorem to classify data (can be images, text, speech or any other form of data). We're interested in finding the probability of a label given the observations made, which we can write as $P(L~|~{D})$.\n",
        "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
        "\n",
        "$$\n",
        "P(L~|~{\\rm D}) = \\frac{P({\\rm D}~|~L)P(L)}{P({\\rm D})}\n",
        "$$\n",
        "\n",
        "If we have two classes (two labels), let them be denoted by $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
        "\n",
        "$$\n",
        "\\frac{P(L_1~|~{\\rm D})}{P(L_2~|~{\\rm D})} = \\frac{P({\\rm D}~|~L_1)}{P({\\rm D}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
        "$$\n",
        "\n",
        "All we need now is some model by which we can compute $P({\\rm D}~|~L_i)$ for each label.\n",
        "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
        "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
        "The general version of such a training step is a very difficult task, but we can make it simpler through a \"Naive\" assumption that each label is associated with a speicfic distribution (often a gaussian).\n",
        "\n",
        "Let us now check on how to use a Naive bayes classifier. We first load the IRIS dataset.Please note that the same dataset will also be used to train a decision tree in the next part of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlXuqwPNXDEa"
      },
      "source": [
        "# Preparing the data into X (predictor) and Y (target).\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Considering two features, the sepal length and width.\n",
        "# X is the input\n",
        "X = iris.data[:, 2:]\n",
        "\n",
        "# y is the target\n",
        "y = iris.target\n",
        "\n",
        "# Creating train and test splits in the data\n",
        "# Train split will be used to train the model\n",
        "train_split_X, train_split_y = X[0:round(len(X)*0.7)], y[0:round(len(y)*0.7)]\n",
        "\n",
        "# The test split will be used to check the accuracy on the test set.\n",
        "test_split_X, test_split_y = X[round(len(X)*0.7):], y[round(len(y)*0.7):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(len(X)*0.7)"
      ],
      "metadata": {
        "id": "UzEma3HJd2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "zT__gLLodjlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTsqkm8lVugH"
      },
      "source": [
        "# Defining the naive bayes model. In this we assume the distribution associated with a label is a gaussian one.\n",
        "model = GaussianNB()\n",
        "\n",
        "# Training the model on the train split\n",
        "model.fit(train_split_X, train_split_y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRyAqtOwJfvo"
      },
      "source": [
        "# Calculating the accuracy of the model\n",
        "acc = model.score(test_split_X, test_split_y)\n",
        "print (\"Accuracy of a pure Naive Bayes Classifier: \", acc*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BGAx6KV-_UIO"
      },
      "source": [
        "## When to Use Naive Bayes\n",
        "\n",
        "Naive Bayesian classifiers uses stringent assumptions about data hence they may not perform that well as a more complicated model.\n",
        "the main advantages they offer are:\n",
        "\n",
        "- They are extremely fast for both training and prediction\n",
        "- They provide straightforward probabilistic prediction\n",
        "- They are often very easily interpretable\n",
        "- They have very few tunable parameters\n",
        "\n",
        "These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. If it performs suitably, then it will be preferred over others due to its advantages, else look for better models!\n",
        "\n",
        "Let us now look at another type of classifiers calles the Decision Trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrpWSbn3BMdv"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYH_PqM_BVd8"
      },
      "source": [
        "Decision Trees are supervised Machine Learning algorithms that can perform both classification and regression tasks and even multioutput tasks. They can handle complex datasets. As the name shows, it uses a tree-like model to make decisions in order to classify or predict according to the problem. It is an ML algorithm that progressively divides datasets into smaller data groups based on a descriptive feature until it reaches sets that are small enough to be described by some label.\n",
        "\n",
        "The importance of decision tree algorithm is that it has many applications in the real world. This is because of its interpretability. Note that while deep learning algorithms can be highly accurate, they are also highly interpretable. This allows applications like:\n",
        "\n",
        "1. In the Healthcare sector: To develop Clinical Decision Analysis tools which allow decision-makers to apply for evidence-based medicine and make objective clinical decisions when faced with complex situations.\n",
        "2. Virtual Assistants (Chatbots): To develop chatbots that provide information and assistance to customers in any required domain.\n",
        "3. Retail and Marketing: Sentiment analysis detects the pulse of customer feedback and emotions and allows organizations to learn about customer choices and drives decisions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXvRD-s1Sp2Z"
      },
      "source": [
        "#### How can an algorithm be represented as a tree?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRGdQfVSp2b"
      },
      "source": [
        "![Image](https://i.ibb.co/rwBRM7B/decistion-tree.jpg)\n",
        "\n",
        "$\\hspace{8cm} \\text {Figure 1: Basic Structure of Decision Tree}$\n",
        "\n",
        "For this, let us see the basic example of the [titanic dataset](https://data.world/nrippner/titanic-disaster-dataset) which predicts whether a passenger survives or not. The below tree uses 3 attributes from the dataset, namely sex, age, and sibsp (Number of Siblings/Spouses Aboard).\n",
        "\n",
        "![Image](https://miro.medium.com/max/360/1*XMId5sJqPtm8-RIwVVz2tg.png)\n",
        "\n",
        "$\\hspace{8cm} \\text {Figure 2: Decision tree using example}$\n",
        "\n",
        "A decision tree is drawn upside down with its root at the top. In the image on the left (is age $>$ 9.5), the bold text in black represents a condition/internal node, based on which the tree splits into branches/ edges. The end of the branch that doesn’t split anymore is the decision/leaf, in this case, whether the passenger died or survived, represented as red and green text respectively.\n",
        "\n",
        "To know more about, decision trees, click [here](https://blog.paperspace.com/decision-trees/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9t88BvrU7Pd"
      },
      "source": [
        "### Training and Visualizing a Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcv_xO3gVKuM"
      },
      "source": [
        "The decision trees can be divided, with respect to the target values, into:\n",
        "\n",
        "- Classification trees: used to classify samples, assign to a limited set of values - classes. In Scikit-Learn it is `DecisionTreeClassifier`.\n",
        "- Regression trees: used to assign samples into numerical values within the range. In Scikit-Learn it is `DecisionTreeRegressor`.\n",
        "\n",
        "To understand Decision Trees, let’s just build one and take a look at how it makes predictions. The  following  code  trains  a  DecisionTreeClassifier  on  the  iris  dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ATUnSRXpsc"
      },
      "source": [
        "# Fit the DT classifier with max_depth = 2\n",
        "clf = DecisionTreeClassifier(max_depth = 2, random_state=1234)\n",
        "model = clf.fit(train_split_X, train_split_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy of the decision tree\n",
        "acc = model.score(train_split_X, train_split_y)\n",
        "print (\"Accuracy of a pure DT Classifier: \", acc*100, \"%\")"
      ],
      "metadata": {
        "id": "5k6vYlcNkAwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1YHJuJYGQO"
      },
      "source": [
        "#### Text Representation\n",
        "\n",
        "Visualizing the Decision tree in text using `text.export_text`. This is important when we want to obtain the model information in a text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjbThBGTX0cH"
      },
      "source": [
        "text_representation = tree.export_text(clf)\n",
        "# Display result\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9MRMTLVY3Io"
      },
      "source": [
        "If you want to save it to the file, it can be done with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sYY3gVUYEZB"
      },
      "source": [
        "# Save in figure\n",
        "with open(\"decision_tree.log\", \"w\") as f_out:\n",
        "    f_out.write(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO87jz1AZKpO"
      },
      "source": [
        "#### Visualize Tree with `plot_tree`\n",
        "\n",
        "This feature requires matplotlib library to be installed. It allows us to easily visualize the tree into a figure (without intermediate exporting to graphviz). You can also refer the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) to learn more about `plot_tree`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ103fwdY8tb"
      },
      "source": [
        "# Visualize tree\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "_ = tree.plot_tree(clf, \n",
        "                   feature_names=iris.feature_names,  \n",
        "                   class_names=iris.target_names,\n",
        "                   filled=True)                           # filled = True uses color coding for majority of classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NUry0gta-Gx"
      },
      "source": [
        "# Save the figure to the .png file\n",
        "fig.savefig(\"decision_tree.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcEVvZnLbYPB"
      },
      "source": [
        "#### How Decision Tree Makes Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr6Ximy1Sp3B"
      },
      "source": [
        "In the above example, we see how the Decision tree makes predictions. In this, we need to classify a flower. At depth 0, at the top (root node), this node asks the condition whether the flower’s sepal length $\\leq$ 2.45 cm. If it is, then we move down to the root’s left child node (depth 1, left). Here, it's a leaf node (means it does not have any child nodes), so it does not ask any questions: you can simply look at the predicted class for that node and the  Decision Tree predicts that the flower is an Iris-Setosa (class=setosa).\n",
        "\n",
        "Now, in another case, when the sepal length of a flower is $\\geq$ 2.45cm. We then move down to right the child node (depth 1, right), which is not a leaf node, and ask if sepal width is $\\leq$ 1.75cm. If it is, then the flower is Iris-Versicolor (depth 2, left). If not, it is surely, Iris-Virginica (depth 2, right).\n",
        "\n",
        "On each node, the `samples` represent the number of training instances. For example, 54 samples have sepal length $\\geq$ 2.45cm and sepal width $\\leq$ 1.75cm (depth 2, left). The node's `values` show how many training instances of each class this node applies. For example, at depth 2, right, applies 0 to Iris-Setosa, 1 to Iris-Versicolor, and 45 to Iris-Virginica.\n",
        "\n",
        "At last, the `gini-impurity` measures the node's impurity, if a node is `pure` (gini = 0), means that all the instances belong to a single class. Example, depth 1, left node. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiiXkASuSp3C"
      },
      "source": [
        "##### Gini Impurity and Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i3LhO3rSp3D"
      },
      "source": [
        "In a decision tree, by default `gini_impurity` is used. But, we can use `entropy` by changing the `criterion` hyperparameter to \"`entropy`\". The gini impurity is one of the methods used in decision tree algorithms to decide the optimal split from a root node and subsequent splits. \n",
        "\n",
        "The entropy came from thermodynamics, where if entropy approaches 0 that means the molecules are still and well-ordered. In ML, it is used to measure the impurity: a set's entropy 0 means that all the instances belong to the same class.\n",
        "\n",
        "*Equation for calculating the Gini-impurity:*\n",
        "\n",
        "$G_i =$ $1 - \\sum_{k=1}^{n} p_{i,k}^2$ \n",
        "\n",
        "The above equation shows how the training algorithm computes the gini-score $G_i$ of the $i^{th}$ node. \n",
        "\n",
        "For example, the *depth-2 left node* has a gini score equal  to $1 – (0/54)^2 – (49/54)^2 – (5/54)^2 ≈ 0.168$.\n",
        "\n",
        "*Equation for calculating the Entropy:*\n",
        "\n",
        "$H_i =$ $- \\sum_{i=1}^{n} p_{i,k} \\log_2 (p_{i,k})$, $\\hspace{0.5cm}where, p_{i,k} \\neq 0$\n",
        "\n",
        "The above equation shows the definition  of  the  entropy  of  the  $i^{th}$  node.\n",
        "\n",
        "For  example, given above, the *depth-2 left node* has an entropy equal to $- \\frac{49}{54} \\log_2 (\\frac{49}{54}) - \\frac{5}{54} \\log_2 (\\frac{5}{54}) \\approx 0.445$\n",
        "\n",
        "Where, $k$ is class of the problem. In this example, there are three classes, Iris-versicolor, Iris-Setosa, and Iris-Virginica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aedC-2UGSp3F"
      },
      "source": [
        "#### Estimating Class Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B9SyG9jSp3G"
      },
      "source": [
        "The instance probability denoted by $p_{i,k}$ denotes the probability of an instance belongs to a class $k$. \n",
        "\n",
        "For example, we have a flower whose sepal length is 5cm and sepal width is 1.5cm (depth 2, left node). The following probabilities: 0% for Iris-Setosa $(\\frac{0}{54})$, 90.74% for Iris-Versicolor $(\\frac{49}{54})$, and 9.25% for Iris-Verginica $(\\frac{5}{54})$. And this will predict the class as Iris-Versicolor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iXqQvXJSp3G"
      },
      "source": [
        "# Predicting probability of a class\n",
        "clf.predict_proba([[5, 1.5]])          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSowh_YSp3H"
      },
      "source": [
        "# Predicting a class\n",
        "clf.predict([[5, 1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0crxUzXSp3I"
      },
      "source": [
        "### The CART Training Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IM-dqeGSp3J"
      },
      "source": [
        "Scikit-Learn  uses  the  Classification  And  Regression  Tree  (CART)  algorithm  to  train Decision Trees (also called “growing” trees). CART constructs binary trees using the feature and threshold that yield the largest information gain or minimum gini-impurity at each node.\n",
        "\n",
        "*Cart Cost Function For Classification*\n",
        "\n",
        "$J_{k,t_k} =$ $\\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}$\n",
        "\n",
        "\\begin{equation*}\n",
        "Where\\begin{cases}\n",
        "         G_{left/right} \\hspace{0.25cm} \\text{measures the  impurity  of  the  left/right  subset}, \\\\\n",
        "         m_{left/right} \\hspace{0.25cm} \\text{is  the  number  of  instances  in  the  left/right  subset}  \\\\\n",
        "     \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Equation shows that the algorithm first the splits the training subsets using a single feature $k$ and a limiting value or condition $t_k$. The algorithm works as follows:\n",
        "\n",
        "- It creates different pairs of $(k,t_k)$. It searches for the pair that produces the purest subset (i.e.minimum gini-impurity) weighted by their size.\n",
        "\n",
        "Once it splits the training set in  two, it splits the subsets using the same algorithm and this iteration will be done until it reaches the maximum depth (i.e., `max_depth` hyperparameter). A  few  other  hyperparameters control   additional  stopping   conditions (`min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIXelwLbSp3U"
      },
      "source": [
        "### Regularization Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9lXP1cBSp3a"
      },
      "source": [
        "Decision tree does not make any prior assumptions of training data (unlike linear models, which assume the data as linear).  If left unconstrained, the tree will closely fit itself according to the training instances, and most likely lead to overfitting (means the model will perform poorly on the test dataset). So, to avoid overfitting the training data, the hyperparameters need to be restricted and passed during the modeling. This process is what is known as Regularization Hyperparameters. \n",
        "\n",
        "In  Scikit-Learn,  this  is  controlled  by  the `max_depth`  hyperparameter  (the  `default  value`  is  `None`,  which  means  unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting. The `DecisionTreeClassifier` class has a few other parameters that similarly restrict the  shape  of  the  Decision  Tree:  `min_samples_split` (the  minimum  number  of  samples a node must have before it can be split), `min_samples_leaf` (the minimum number   of   samples   a   leaf   node   must   have),   `min_weight_fraction_leaf`   (same   as min_samples_leaf  but  expressed  as  a  fraction  of  the  total  number  of  weighted instances), `max_leaf_nodes` (maximum  number  of  leaf  nodes),  and  `max_features` (maximum number of features that are evaluated for splitting at each node). Increasing  `min_*`  hyperparameters  or  reducing  `max_*`  hyperparameters  will  regularize  the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcgKcBfSp3b"
      },
      "source": [
        "In the example below, we will visualize the decision tree, one by regularizing hyperparameter and one without any restrictions.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58y7KMn5Sp3c"
      },
      "source": [
        "# Load iris dataset and define variables with sepal length and width\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, 2:] \n",
        "y = iris.target\n",
        "\n",
        "# First tree without restrictions\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "# Second tree with hyperparameters\n",
        "tree_clf2 = DecisionTreeClassifier(max_depth =2, min_samples_leaf =1, min_samples_split = 2, random_state=2)\n",
        "tree_clf2.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize tree\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "_ = tree.plot_tree(tree_clf, \n",
        "                   feature_names=iris.feature_names,  \n",
        "                   class_names=iris.target_names,\n",
        "                   filled=True)"
      ],
      "metadata": {
        "id": "E8nVWkk7nwqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize tree\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "_ = tree.plot_tree(tree_clf2, \n",
        "                   feature_names=iris.feature_names,  \n",
        "                   class_names=iris.target_names,\n",
        "                   filled=True)"
      ],
      "metadata": {
        "id": "NhfKIrXvn226"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHGevdGrSp3e"
      },
      "source": [
        "# Define a function for plotting decision boundary\n",
        "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris = True, legend=False, plot_training=True):\n",
        "\n",
        "    # define array for x1 and x2 axes\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)                                                                \n",
        "    x2s = np.linspace(axes[2], axes[3], 100) \n",
        "    \n",
        "    # make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids                                                               \n",
        "    x1, x2 = np.meshgrid(x1s, x2s)         \n",
        "\n",
        "    # the numpy.ravel() functions returns contiguous flattened array(1D array with all the input-array elements and with the same type as it)                                                                 \n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]                                       \n",
        "    # predict and reshape the y_pred according to x\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)  \n",
        "\n",
        "    # module is used for mapping numbers to colors or color specification conversion in a 1-D array of colors also known as colormap\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])               \n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)                         \n",
        "  \n",
        "    if plot_training:\n",
        "        # plot Setosa in yellow\n",
        "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")       \n",
        "        # plot Versicolor in blue  \n",
        "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")   \n",
        "        # plot Virginica in green\n",
        "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")    \n",
        "        plt.axis(axes)\n",
        "\n",
        "    if iris:\n",
        "        # define x_axes label\n",
        "        plt.xlabel(\"Sepal length\", fontsize=14)                                 \n",
        "        # define y_axes label\n",
        "        plt.ylabel(\"Sepal width\", fontsize=14)                                  \n",
        "    \n",
        "    if legend:\n",
        "        plt.legend(loc=\"lower right\", fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxvhbBPtSp3f"
      },
      "source": [
        "To know more about Listedcolormap and meshgrid in the above code, click [here](https://stackoverflow.com/questions/44443993/matplotlib-colors-listedcolormap-in-python)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15k2xWeMSp3g"
      },
      "source": [
        "# Plot both the decision tree\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# call the plot_decision_boundary function for tree_clf\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
        "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
        "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
        "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
        "plt.title(\"No restrictions\", fontsize=16)\n",
        "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
        "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
        "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# call the plot_decision_boundary function for tree_clf2\n",
        "plot_decision_boundary(tree_clf2, X, y)\n",
        "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
        "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
        "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
        "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
        "plt.title(\"Regularizing Hyperparameters\", fontsize=16)\n",
        "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
        "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
        "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXsrHQETSp3h"
      },
      "source": [
        "From the above figure, we can see that, when there is no restriction of hyperparameters in decision tree, it can adjust itself according to the training dataset (overfitting problem). While, on the other hand, where there is the regularization of hyperparameters, the model will probably generalize better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhDq_0Loq_S"
      },
      "source": [
        "### Ungraded Excercises\n",
        "\n",
        "1. Compare the Naive Bayes and Decision Tree classifier, based on complexity and speed.\n",
        "2. Use MNIST dataset to train both of the classifiers and compare it with other linear classifiers that you have learned.\n",
        "\n",
        "\n",
        "### Graded Excercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title Is Decision Tree a classification or regression model? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"classification model\", \"regression model\", \"Both\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}